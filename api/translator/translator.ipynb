{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%matplotlib inline\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import tensorflow as tf\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import pickle\r\n",
    "import math\r\n",
    "import re\r\n",
    "import os"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from tensorflow.keras.models import Model\r\n",
    "from tensorflow.keras.layers import Input, Dense, GRU, Embedding\r\n",
    "from tensorflow.keras.optimizers import RMSprop\r\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\r\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\"\"\r\n",
    "Original author is Magnus Erik Hvass Pedersen\r\n",
    "\r\n",
    "https://github.com/Hvass-Labs/TensorFlow-Tutorials\r\n",
    "\r\n",
    "https://www.youtube.com/watch?v=vI2Y3I-JI2Q&t=1824s&ab_channel=HvassLaboratories\r\n",
    "\r\n",
    "License(MIT)\r\n",
    "These tutorials and source-code are published under the MIT License which allows very broad use for both academic and commercial purposes.\r\n",
    "\r\n",
    "A few of the images used for demonstration purposes may be under copyright. These images are included under the \"fair usage\" laws.\r\n",
    "\r\n",
    "You are very welcome to modify these tutorials and use them in your own projects. Please keep a link to the original repository.\r\n",
    "\"\"\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "tf.__version__"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'2.6.0'"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "tf.keras.__version__"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'2.6.0'"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from tensorflow.python.client import device_lib\r\n",
    "print(device_lib.list_local_devices())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 254509895096962513\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 2254123828\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 8809781079984874963\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce GTX 1650 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      "]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "df  = pd.read_csv('en-ig.csv',sep=';')"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# type(df[\"english\"].tolist())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "language_code='da'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# df = df.to_nump()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "mark_start = 'ssss '\r\n",
    "mark_end = ' eeee'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "data_src = df[\"yoruba\"].tolist()\r\n",
    "\r\n",
    "data_dest = df[\"english\"].tolist()\r\n",
    "\r\n",
    "\r\n",
    "for i in range(len(data_src)):\r\n",
    "    #re.sub('[^a-zA-Z0-9 ]+', '', str(data_src[i]))\r\n",
    "    data_src[i]=str(data_src[i])\r\n",
    "\r\n",
    "\r\n",
    "for i in range(len(data_dest)):\r\n",
    "    #data_dest[i]=mark_start+str(data_dest[i])+mark_end\r\n",
    "    data_dest[i]=mark_start+str(data_dest[i])+mark_end\r\n",
    "    \r\n",
    "print(str(data_dest[6]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ssss Tottenham emerila n'asọmpị iko FA ugboro asatọ eeee\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "idx = 2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# data_src = data_src[:10000]\r\n",
    "# print(len(data_src))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# data_dest = data_dest[:10000]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# idx = 8002"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "#data_src[idx]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "# data_dest[idx]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "num_words = 10000"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "class TokenizerWrap(Tokenizer):\r\n",
    "    \"\"\"Wrap the Tokenizer-class from Keras with more functionality.\"\"\"\r\n",
    "    \r\n",
    "    def __init__(self, texts, padding,\r\n",
    "                 reverse=False, num_words=None):\r\n",
    "        \"\"\"\r\n",
    "        :param texts: List of strings. This is the data-set.\r\n",
    "        :param padding: Either 'post' or 'pre' padding.\r\n",
    "        :param reverse: Boolean whether to reverse token-lists.\r\n",
    "        :param num_words: Max number of words to use.\r\n",
    "        \"\"\"\r\n",
    "\r\n",
    "        Tokenizer.__init__(self, num_words=num_words)\r\n",
    "\r\n",
    "        # Create the vocabulary from the texts.\r\n",
    "        self.fit_on_texts(texts)\r\n",
    "\r\n",
    "        # Create inverse lookup from integer-tokens to words.\r\n",
    "        self.index_to_word = dict(zip(self.word_index.values(),\r\n",
    "                                      self.word_index.keys()))\r\n",
    "\r\n",
    "        # Convert all texts to lists of integer-tokens.\r\n",
    "        # Note that the sequences may have different lengths.\r\n",
    "        self.tokens = self.texts_to_sequences(texts)\r\n",
    "\r\n",
    "        if reverse:\r\n",
    "            # Reverse the token-sequences.\r\n",
    "            self.tokens = [list(reversed(x)) for x in self.tokens]\r\n",
    "        \r\n",
    "            # Sequences that are too long should now be truncated\r\n",
    "            # at the beginning, which corresponds to the end of\r\n",
    "            # the original sequences.\r\n",
    "            truncating = 'pre'\r\n",
    "        else:\r\n",
    "            # Sequences that are too long should be truncated\r\n",
    "            # at the end.\r\n",
    "            truncating = 'post'\r\n",
    "\r\n",
    "        # The number of integer-tokens in each sequence.\r\n",
    "        self.num_tokens = [len(x) for x in self.tokens]\r\n",
    "\r\n",
    "        # Max number of tokens to use in all sequences.\r\n",
    "        # We will pad / truncate all sequences to this length.\r\n",
    "        # This is a compromise so we save a lot of memory and\r\n",
    "        # only have to truncate maybe 5% of all the sequences.\r\n",
    "        self.max_tokens = np.mean(self.num_tokens) \\\r\n",
    "                          + 2 * np.std(self.num_tokens)\r\n",
    "        self.max_tokens = int(self.max_tokens)\r\n",
    "\r\n",
    "        # Pad / truncate all token-sequences to the given length.\r\n",
    "        # This creates a 2-dim numpy matrix that is easier to use.\r\n",
    "        self.tokens_padded = pad_sequences(self.tokens,\r\n",
    "                                           maxlen=self.max_tokens,\r\n",
    "                                           padding=padding,\r\n",
    "                                           truncating=truncating)\r\n",
    "\r\n",
    "    def token_to_word(self, token):\r\n",
    "        \"\"\"Lookup a single word from an integer-token.\"\"\"\r\n",
    "\r\n",
    "        word = \" \" if token == 0 else self.index_to_word[token]\r\n",
    "        return word \r\n",
    "\r\n",
    "    def tokens_to_string(self, tokens):\r\n",
    "        \"\"\"Convert a list of integer-tokens to a string.\"\"\"\r\n",
    "\r\n",
    "        # Create a list of the individual words.\r\n",
    "        words = [self.index_to_word[token]\r\n",
    "                 for token in tokens\r\n",
    "                 if token != 0]\r\n",
    "        \r\n",
    "        # Concatenate the words to a single string\r\n",
    "        # with space between all the words.\r\n",
    "        text = \" \".join(words)\r\n",
    "\r\n",
    "        return text\r\n",
    "    \r\n",
    "    def text_to_tokens(self, text, reverse=False, padding=False):\r\n",
    "        \"\"\"\r\n",
    "        Convert a single text-string to tokens with optional\r\n",
    "        reversal and padding.\r\n",
    "        \"\"\"\r\n",
    "\r\n",
    "        # Convert to tokens. Note that we assume there is only\r\n",
    "        # a single text-string so we wrap it in a list.\r\n",
    "        tokens = self.texts_to_sequences([text])\r\n",
    "        tokens = np.array(tokens)\r\n",
    "\r\n",
    "        if reverse:\r\n",
    "            # Reverse the tokens.\r\n",
    "            tokens = np.flip(tokens, axis=1)\r\n",
    "\r\n",
    "            # Sequences that are too long should now be truncated\r\n",
    "            # at the beginning, which corresponds to the end of\r\n",
    "            # the original sequences.\r\n",
    "            truncating = 'pre'\r\n",
    "        else:\r\n",
    "            # Sequences that are too long should be truncated\r\n",
    "            # at the end.\r\n",
    "            truncating = 'post'\r\n",
    "\r\n",
    "        if padding:\r\n",
    "            # Pad and truncate sequences to the given length.\r\n",
    "            tokens = pad_sequences(tokens,\r\n",
    "                                   maxlen=self.max_tokens,\r\n",
    "                                   padding='pre',\r\n",
    "                                   truncating=truncating)\r\n",
    "\r\n",
    "        return tokens"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "%%time\r\n",
    "tokenizer_src = TokenizerWrap(texts=data_src,\r\n",
    "                              padding='pre',\r\n",
    "                              reverse=True,\r\n",
    "                              num_words=num_words)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 293 ms\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "%%time\r\n",
    "tokenizer_dest = TokenizerWrap(texts=data_dest,\r\n",
    "                               padding='post',\r\n",
    "                               reverse=False,\r\n",
    "                               num_words=num_words)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 505 ms\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "tokens_src = tokenizer_src.tokens_padded\r\n",
    "tokens_dest = tokenizer_dest.tokens_padded\r\n",
    "print(tokens_src.shape)\r\n",
    "print(tokens_dest.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(10000, 39)\n",
      "(10000, 45)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "token_start = tokenizer_dest.word_index[mark_start.strip()]\r\n",
    "token_start"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "token_end = tokenizer_dest.word_index[mark_end.strip()]\r\n",
    "token_end"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "idx = 2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "tokens_src[idx]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0, 4002,    5,  232,\n",
       "        230,  432,   17,   14,   34,  904])"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "data_src[idx]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Dapchi: Government has not defeated Boko Haram and Massob'"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "tokens_dest[idx]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([   2,  881,  152, 7433,  298,  302, 3211,    3,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0])"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "encoder_input_data = tokens_src"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "decoder_input_data = tokens_dest[:, :-1]\r\n",
    "decoder_input_data.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(10000, 44)"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "decoder_output_data = tokens_dest[:, 1:]\r\n",
    "decoder_output_data.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(10000, 44)"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "idx = 2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "decoder_input_data[idx]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([   2,  881,  152, 7433,  298,  302, 3211,    3,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "decoder_output_data[idx]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 881,  152, 7433,  298,  302, 3211,    3,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "tokenizer_dest.tokens_to_string(decoder_input_data[idx])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'ssss dapchi gọọmenti emeribeghị boko haram massob eeee'"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "tokenizer_dest.tokens_to_string(decoder_output_data[idx])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'dapchi gọọmenti emeribeghị boko haram massob eeee'"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "encoder_input = Input(shape=(None, ), name='encoder_input')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "embedding_size = 128"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "encoder_embedding = Embedding(input_dim=num_words,\r\n",
    "                              output_dim=embedding_size,\r\n",
    "                              name='encoder_embedding')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is the size of the internal states of the Gated Recurrent Units (GRU). The same size is used in both the encoder and decoder."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "state_size = 512"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This creates the 3 GRU layers that will map from a sequence of embedding-vectors to a single \"thought vector\" which summarizes the contents of the input-text. Note that the last GRU-layer does not return a sequence."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "encoder_gru1 = GRU(state_size, name='encoder_gru1',\r\n",
    "                   return_sequences=True)\r\n",
    "encoder_gru2 = GRU(state_size, name='encoder_gru2',\r\n",
    "                   return_sequences=True)\r\n",
    "encoder_gru3 = GRU(state_size, name='encoder_gru3',\r\n",
    "                   return_sequences=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This helper-function connects all the layers of the encoder."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "def connect_encoder():\r\n",
    "    # Start the neural network with its input-layer.\r\n",
    "    net = encoder_input\r\n",
    "    \r\n",
    "    # Connect the embedding-layer.\r\n",
    "    net = encoder_embedding(net)\r\n",
    "\r\n",
    "    # Connect all the GRU-layers.\r\n",
    "    net = encoder_gru1(net)\r\n",
    "    net = encoder_gru2(net)\r\n",
    "    net = encoder_gru3(net)\r\n",
    "\r\n",
    "    # This is the output of the encoder.\r\n",
    "    encoder_output = net\r\n",
    "    \r\n",
    "    return encoder_output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "encoder_output = connect_encoder()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "decoder_initial_state = Input(shape=(state_size,),\r\n",
    "                              name='decoder_initial_state')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "decoder_input = Input(shape=(None, ), name='decoder_input')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "decoder_embedding = Embedding(input_dim=num_words,\r\n",
    "                              output_dim=embedding_size,\r\n",
    "                              name='decoder_embedding')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "decoder_gru1 = GRU(state_size, name='decoder_gru1',\r\n",
    "                   return_sequences=True)\r\n",
    "decoder_gru2 = GRU(state_size, name='decoder_gru2',\r\n",
    "                   return_sequences=True)\r\n",
    "decoder_gru3 = GRU(state_size, name='decoder_gru3',\r\n",
    "                   return_sequences=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "decoder_dense = Dense(num_words,\r\n",
    "                      activation='softmax',\r\n",
    "                      name='decoder_output')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "def connect_decoder(initial_state):\r\n",
    "    # Start the decoder-network with its input-layer.\r\n",
    "    net = decoder_input\r\n",
    "\r\n",
    "    # Connect the embedding-layer.\r\n",
    "    net = decoder_embedding(net)\r\n",
    "    \r\n",
    "    # Connect all the GRU-layers.\r\n",
    "    net = decoder_gru1(net, initial_state=initial_state)\r\n",
    "    net = decoder_gru2(net, initial_state=initial_state)\r\n",
    "    net = decoder_gru3(net, initial_state=initial_state)\r\n",
    "\r\n",
    "    # Connect the final dense layer that converts to\r\n",
    "    # one-hot encoded arrays.\r\n",
    "    decoder_output = decoder_dense(net)\r\n",
    "    \r\n",
    "    return decoder_output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "decoder_output = connect_decoder(initial_state=encoder_output)\r\n",
    "\r\n",
    "model_train = Model(inputs=[encoder_input, decoder_input],\r\n",
    "                    outputs=[decoder_output])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "model_encoder = Model(inputs=[encoder_input],\r\n",
    "                      outputs=[encoder_output])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "decoder_output = connect_decoder(initial_state=decoder_initial_state)\r\n",
    "\r\n",
    "model_decoder = Model(inputs=[decoder_input, decoder_initial_state],\r\n",
    "                      outputs=[decoder_output])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "model_train.compile(optimizer=RMSprop(learning_rate=1e-3),\r\n",
    "                    loss='sparse_categorical_crossentropy')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "path_checkpoint = '21_checkpoint.keras'\r\n",
    "callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\r\n",
    "                                      monitor='val_loss',\r\n",
    "                                      verbose=1,\r\n",
    "                                      save_weights_only=True,\r\n",
    "                                      save_best_only=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "callback_early_stopping = EarlyStopping(monitor='val_loss',\r\n",
    "                                        patience=3, verbose=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "callback_tensorboard = TensorBoard(log_dir='./21_logs/',\r\n",
    "                                   histogram_freq=0,\r\n",
    "                                   write_graph=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "callbacks = [callback_early_stopping,\r\n",
    "             callback_checkpoint,\r\n",
    "             callback_tensorboard]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "try:\r\n",
    "    model_train.load_weights(path_checkpoint)\r\n",
    "except Exception as error:\r\n",
    "    print(\"Error trying to load checkpoint.\")\r\n",
    "    print(error)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "x_data = \\\r\n",
    "{\r\n",
    "    'encoder_input': encoder_input_data,\r\n",
    "    'decoder_input': decoder_input_data\r\n",
    "}\r\n",
    "\r\n",
    "print(len(encoder_input_data))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "y_data = \\\r\n",
    "{\r\n",
    "    'decoder_output': decoder_output_data\r\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "validation_split = 500 / len(encoder_input_data)\r\n",
    "validation_split"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.05"
      ]
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "model_train.fit(x=x_data,\r\n",
    "                y=y_data,\r\n",
    "                batch_size=90,\r\n",
    "                epochs=10,\r\n",
    "                validation_split=0.12,\r\n",
    "                callbacks=callbacks)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "98/98 [==============================] - 29s 226ms/step - loss: 1.6109 - val_loss: 3.1855\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.18554, saving model to 21_checkpoint.keras\n",
      "Epoch 2/10\n",
      "98/98 [==============================] - 21s 215ms/step - loss: 1.5011 - val_loss: 3.2161\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 3.18554\n",
      "Epoch 3/10\n",
      "98/98 [==============================] - 21s 216ms/step - loss: 1.4281 - val_loss: 3.2389\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 3.18554\n",
      "Epoch 4/10\n",
      "98/98 [==============================] - 21s 216ms/step - loss: 1.3638 - val_loss: 3.2509\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 3.18554\n",
      "Epoch 00004: early stopping\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19b6975e640>"
      ]
     },
     "metadata": {},
     "execution_count": 72
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "with open('token_src.pickle','wb') as f:\r\n",
    "    pickle.dump(tokenizer_src,f)\r\n",
    "    \r\n",
    "with open('token_dest.pickle','wb') as f:\r\n",
    "    pickle.dump(tokenizer_dest,f)\r\n",
    "    \r\n",
    "# model_encoder.save(\"model_e\")\r\n",
    "# model_decoder.save('model_d')\r\n",
    "tf.keras.models.save_model(model_encoder, \"model_e2\")\r\n",
    "tf.keras.models.save_model(model_decoder, \"model_d2\")\r\n",
    "     #tf.keras.models.save_model()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_2_layer_call_fn while saving (showing 5 of 15). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: model_e2\\assets\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:Assets written to: model_e2\\assets\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Found untraced functions such as gru_cell_3_layer_call_fn, gru_cell_3_layer_call_and_return_conditional_losses, gru_cell_4_layer_call_fn, gru_cell_4_layer_call_and_return_conditional_losses, gru_cell_5_layer_call_fn while saving (showing 5 of 15). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: model_d2\\assets\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:Assets written to: model_d2\\assets\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "def translate(input_text, true_output_text=None):\r\n",
    "    \"\"\"Translate a single text-string.\"\"\"\r\n",
    "\r\n",
    "    # Convert the input-text to integer-tokens.\r\n",
    "    # Note the sequence of tokens has to be reversed.\r\n",
    "    # Padding is probably not necessary.\r\n",
    "    #     tkn = TokenizerWrap(texts=input_text,\r\n",
    "    #                               padding='pre',\r\n",
    "    #                               reverse=True)\r\n",
    "    \r\n",
    "    \r\n",
    "    input_tokens = tokenizer_src.text_to_tokens(text=input_text,\r\n",
    "                                                reverse=True,\r\n",
    "                                                padding=True)\r\n",
    "    \r\n",
    "    # Get the output of the encoder's GRU which will be\r\n",
    "    # used as the initial state in the decoder's GRU.\r\n",
    "    # This could also have been the encoder's final state\r\n",
    "    # but that is really only necessary if the encoder\r\n",
    "    # and decoder use the LSTM instead of GRU because\r\n",
    "    # the LSTM has two internal states.\r\n",
    "    print('THE MODEL IS',model_encoder)\r\n",
    "    \r\n",
    "    \r\n",
    "    initial_state = model_encoder.predict(input_tokens)\r\n",
    "    #print('THIS IS THE STATE',initial_state)\r\n",
    "\r\n",
    "    # Max number of tokens / words in the output sequence.\r\n",
    "    max_tokens = tokenizer_dest.max_tokens\r\n",
    "\r\n",
    "    # Pre-allocate the 2-dim array used as input to the decoder.\r\n",
    "    # This holds just a single sequence of integer-tokens,\r\n",
    "    # but the decoder-model expects a batch of sequences.\r\n",
    "    shape = (1, max_tokens)\r\n",
    "    decoder_input_data = np.zeros(shape=shape, dtype=np.int)\r\n",
    "\r\n",
    "    # The first input-token is the special start-token for 'ssss '.\r\n",
    "    token_int = token_start\r\n",
    "\r\n",
    "    # Initialize an empty output-text.\r\n",
    "    output_text = ''\r\n",
    "\r\n",
    "    # Initialize the number of tokens we have processed.\r\n",
    "    count_tokens = 0\r\n",
    "\r\n",
    "    # While we haven't sampled the special end-token for ' eeee'\r\n",
    "    # and we haven't processed the max number of tokens.\r\n",
    "    while token_int != token_end and count_tokens < max_tokens:\r\n",
    "        # Update the input-sequence to the decoder\r\n",
    "        # with the last token that was sampled.\r\n",
    "        # In the first iteration this will set the\r\n",
    "        # first element to the start-token.\r\n",
    "        decoder_input_data[0, count_tokens] = token_int\r\n",
    "\r\n",
    "        # Wrap the input-data in a dict for clarity and safety,\r\n",
    "        # so we are sure we input the data in the right order.\r\n",
    "        x_data = \\\r\n",
    "        {\r\n",
    "            'decoder_initial_state': initial_state,\r\n",
    "            'decoder_input': decoder_input_data\r\n",
    "        }\r\n",
    "\r\n",
    "        # Note that we input the entire sequence of tokens\r\n",
    "        # to the decoder. This wastes a lot of computation\r\n",
    "        # because we are only interested in the last input\r\n",
    "        # and output. We could modify the code to return\r\n",
    "        # the GRU-states when calling predict() and then\r\n",
    "        # feeding these GRU-states as well the next time\r\n",
    "        # we call predict(), but it would make the code\r\n",
    "        # much more complicated.\r\n",
    "\r\n",
    "        # Input this data to the decoder and get the predicted output.\r\n",
    "        decoder_output = model_decoder.predict(x_data)\r\n",
    "\r\n",
    "        # Get the last predicted token as a one-hot encoded array.\r\n",
    "        token_onehot = decoder_output[0, count_tokens, :]\r\n",
    "        \r\n",
    "        # Convert to an integer-token.\r\n",
    "        token_int = np.argmax(token_onehot)\r\n",
    "\r\n",
    "        # Lookup the word corresponding to this integer-token.\r\n",
    "        sampled_word = tokenizer_dest.token_to_word(token_int)\r\n",
    "\r\n",
    "        # Append the word to the output-text.\r\n",
    "        output_text += \" \" + sampled_word\r\n",
    "\r\n",
    "        # Increment the token-counter.\r\n",
    "        count_tokens += 1\r\n",
    "\r\n",
    "    # Sequence of tokens output by the decoder.\r\n",
    "    output_tokens = decoder_input_data[0]\r\n",
    "    \r\n",
    "    # Print the input-text.\r\n",
    "    print(\"Input text:\")\r\n",
    "    print(input_text)\r\n",
    "    print()\r\n",
    "\r\n",
    "    # Print the translated output-text.\r\n",
    "    print(\"Translated text:\")\r\n",
    "    print(output_text)\r\n",
    "    print()\r\n",
    "\r\n",
    "    # Optionally print the true translated text.\r\n",
    "    if true_output_text is not None:\r\n",
    "        print(\"True output text:\")\r\n",
    "        print(true_output_text)\r\n",
    "        print()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Examples\n",
    "\n",
    "Translate a text from the training-data. This translation is quite good. Note how it is not identical to the translation from the training-data, but the actual meaning is similar."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "source": [
    "idx = 3\r\n",
    "translate(input_text=data_src[idx],\r\n",
    "          true_output_text=data_dest[idx])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "THE MODEL IS <keras.engine.functional.Functional object at 0x0000019B685A95B0>\n",
      "Input text:\n",
      "Tottenham look forward to lifting the FA cup in defeating Rochadale.\n",
      "\n",
      "Translated text:\n",
      " cheta na mba ofesi na mba ọzọ nwere ike iri iri eeee\n",
      "\n",
      "True output text:\n",
      "ssss Tottenham na-ele anya iburu iko FA na mmeri Rochadale eeee\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here is another example which is also a reasonable translation, although it has incorrectly translated the natural disasters. Note \"countries of the European Union\" has instead been translated as \"member states\" which are synonyms in this context."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "source": [
    "idx = 4\r\n",
    "translate(input_text=data_src[idx],\r\n",
    "          true_output_text=data_dest[idx])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "THE MODEL IS <keras.engine.functional.Functional object at 0x0000019B685A95B0>\n",
      "Input text:\n",
      "Son Heung-min, Fernando Llorente and Kyle Walter-Peters of Tottenham beat Rochadale mercilessly in their yesterday's FA cup competition. \n",
      "\n",
      "Translated text:\n",
      " onye ntaakụkọ na ndị mba amerika bụ ahụike n'aha ịchafụ kwuru na ha ga enye ndị mmadụ naịjirịa na enweta ndị naịjirịa na ndị super eagles eeee\n",
      "\n",
      "True output text:\n",
      "ssss Son Heung-min, Fernando Llorente na Kyle Walter-Peters nke Tottenham bụ ndị mmechiri ọnụ Rochadale n'asọmpị ụnyaahụ. eeee\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this example we join two texts from the training-set. The model first sends this combined text through the encoder, which produces a \"thought-vector\" that seems to summarize both texts reasonably well so the decoder can produce a reasonable translation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "source": [
    "idx = 3\r\n",
    "translate(input_text=data_src[idx] + data_src[idx+1],\r\n",
    "          true_output_text=data_dest[idx] + data_dest[idx+1])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "THE MODEL IS <keras.engine.functional.Functional object at 0x0000019B685A95B0>\n",
      "Input text:\n",
      "Tottenham look forward to lifting the FA cup in defeating Rochadale.Son Heung-min, Fernando Llorente and Kyle Walter-Peters of Tottenham beat Rochadale mercilessly in their yesterday's FA cup competition. \n",
      "\n",
      "Translated text:\n",
      " n'ime nke a bụ nke nwaanyị na akpọ nke a bụ nke nwaanyị a mụrụ kwuo na anọ a na agba anya na oke a na agba nke afọ a na agba nke afọ a na agba nke ọnwa eeee\n",
      "\n",
      "True output text:\n",
      "ssss Tottenham na-ele anya iburu iko FA na mmeri Rochadale eeeessss Son Heung-min, Fernando Llorente na Kyle Walter-Peters nke Tottenham bụ ndị mmechiri ọnụ Rochadale n'asọmpị ụnyaahụ. eeee\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we reverse the order of these two texts then the meaning is not quite so clear for the latter text."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "idx = 3\r\n",
    "translate(input_text=data_src[idx+1] + data_src[idx],\r\n",
    "          true_output_text=data_dest[idx+1] + data_dest[idx])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "THE MODEL IS <keras.engine.functional.Functional object at 0x0000019B685A95B0>\n",
      "Input text:\n",
      "Son Heung-min, Fernando Llorente and Kyle Walter-Peters of Tottenham beat Rochadale mercilessly in their yesterday's FA cup competition. Tottenham look forward to lifting the FA cup in defeating Rochadale.\n",
      "\n",
      "Translated text:\n",
      " onye na emepụta ọnụ na mba amerika bụ ike na ndị na emepụta ọrịa na mba ọrụ ahụ na enye ụmụ egwuregwu bọọlụ iri iri iri iri iri iri iri iri iri iri iri iri iri iri iri iri iri iri iri iri iri iri\n",
      "\n",
      "True output text:\n",
      "ssss Son Heung-min, Fernando Llorente na Kyle Walter-Peters nke Tottenham bụ ndị mmechiri ọnụ Rochadale n'asọmpị ụnyaahụ. eeeessss Tottenham na-ele anya iburu iko FA na mmeri Rochadale eeee\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is an example I made up. It is a quite broken translation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "source": [
    "translate(input_text=\"i like you\",\r\n",
    "          true_output_text='how are you')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "THE MODEL IS <keras.engine.functional.Functional object at 0x0000019B685A95B0>\n",
      "Input text:\n",
      "i like you\n",
      "\n",
      "Translated text:\n",
      " ihe nkiri eeee\n",
      "\n",
      "True output text:\n",
      "how are you\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "source": [
    "translate(input_text=\"The news that will interest you\",\r\n",
    "          true_output_text='how are you')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "THE MODEL IS <keras.engine.functional.Functional object at 0x0000019B685A95B0>\n",
      "Input text:\n",
      "The news that will interest you\n",
      "\n",
      "Translated text:\n",
      " akụkọ ga amasị gị eeee\n",
      "\n",
      "True output text:\n",
      "how are you\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is another example I made up. This is a better translation even though it is perhaps a more complicated text."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "source": [
    "translate(input_text=\"They came and arrested some of them.\",\r\n",
    "          true_output_text=\"Ha bịara nwụchie ụfọdụ n'ime ha.\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "THE MODEL IS <keras.engine.functional.Functional object at 0x0000019B685A95B0>\n",
      "Input text:\n",
      "They came and arrested some of them.\n",
      "\n",
      "Translated text:\n",
      " ha na agba bọọlụ na if dị na mba swidin eeee\n",
      "\n",
      "True output text:\n",
      "Ha bịara nwụchie ụfọdụ n'ime ha.\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is a text from a Danish song. It doesn't even make much sense in Danish. However the translation is probably so broken because several of the words are not in the vocabulary."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "translate(input_text=\"Hvem spæner ud af en butik og tygger de stærkeste bolcher?\",\r\n",
    "          true_output_text=\"Who runs out of a shop and chews the strongest bon-bons?\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "THE MODEL IS <keras.engine.functional.Functional object at 0x0000019B685A95B0>\n",
      "Input text:\n",
      "Hvem spæner ud af en butik og tygger de stærkeste bolcher?\n",
      "\n",
      "Translated text:\n",
      " n'ofesi eeee\n",
      "\n",
      "True output text:\n",
      "Who runs out of a shop and chews the strongest bon-bons?\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}